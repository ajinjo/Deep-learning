{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f834a40",
   "metadata": {},
   "source": [
    "## 자연어 처리 (Natural Language Processing : NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3773eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "자연어 처리\n",
    "- 음성이나 텍스트를 컴퓨터가 인식하고 처리하는 것\n",
    "\n",
    "컴퓨터를 이용해서 인간의 언어를 이해하는 연구는 딥러닝 이전부터 지속\n",
    "딥러닝이 등장하면서 자연어 처리 연구 활발\n",
    "대용량 데이터를 이용해서 자연어 데이터를 지속적으로 입력해\n",
    "끊임 없이 학습하는 가능해졌기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "자연어 처리\n",
    "- 텍스트 자료는 딥러닝에서 그대로 입력할 수 없음\n",
    "- 컴퓨터 알고리즘은 수치로 된 데이터만 이해할 뿐 텍스트 자체를 이해할 수 없기 때문\n",
    "- 텍스트를 정제하는 전처리 과정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "텍스트 전처리 작업\n",
    "\n",
    "토큰화 (Tokenization) : 토큰(token)이라는 단위로 나누는 작업. 단어 토큰화\n",
    "\n",
    "정제 (Cleaning) : 노이즈 데이터 제거\n",
    "    \n",
    "정규화 (Normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 표현의 단어로 만드는 것\n",
    "    \n",
    "어간 추출 (Stemming)과 표제어 추출(Lemmatization)\n",
    "- 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이는 작업\n",
    "\n",
    "불용어 (Stopword) 제거\n",
    "- 문장에서는 자주 등장하지만 \n",
    "- 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 단어 제거\n",
    "\n",
    "정규 표현식 (Regular Expression) : 정규식을 사용해서 특정 규칙이 있는 텍스트 데이터를 빠르게 정제\n",
    "    \n",
    "정수 인코딩 (Integer Encoding)\n",
    "- 각 단어를 고유한 정수에 맵핑 (mapping)\n",
    "- index 부여\n",
    "\n",
    "패딩 (Padding)\n",
    "- 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업\n",
    "- 하나의 행렬로 보고 한꺼번에 묶어서 처리\n",
    "\n",
    "원-핫 인코딩 : 문자를 숫자로 변환\n",
    "- 해당 인덱스에는 1, 그 외는 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8550e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "텍스트 토큰화\n",
    "토큰화 (Tokenization) : 토큰(token)이라는 단위로 나누는 작업. 단어 토큰화\n",
    "- 입력된 텍스트를 잘게 나누는 과정 (단어로 나눔)\n",
    "\n",
    "토큰 (token)\n",
    "- 작게 나누어진 하나의 단위\n",
    "- 단어, 문장, 형태소 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0142f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "# 케라스의 text_to_word_sequence() 함수 사용해서\n",
    "# 텍스트 토큰화\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = '해보지 않으면 해낼 수 없다'\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0535be",
   "metadata": {},
   "outputs": [],
   "source": [
    "단어의 빈도수 확인\n",
    "- 각 단어가 몇 번이나 중복해서 쓰였는지 확인\n",
    "- 단어의 빈도수를 알면 \n",
    "- 텍스트에서 중요한 역할을 하는 단어를 파악할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cdb1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('먼저', 1),\n",
       "             ('텍스트의', 2),\n",
       "             ('각', 1),\n",
       "             ('단어를', 2),\n",
       "             ('나누어', 1),\n",
       "             ('토큰화', 3),\n",
       "             ('합니다', 1),\n",
       "             ('단어로', 1),\n",
       "             ('해야', 1),\n",
       "             ('딥러닝에서', 2),\n",
       "             ('인식합니다', 1),\n",
       "             ('한', 1),\n",
       "             ('결과는', 1),\n",
       "             ('사용할', 1),\n",
       "             ('수', 1),\n",
       "             ('있습니다', 1)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스의 Tokenizer() 클래스를 사용해서 빈도수 계산\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
    "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식합니다.',\n",
    "       '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.']\n",
    "\n",
    "token = Tokenizer()      # 토큰화 객체 생성\n",
    "token.fit_on_texts(docs) # 문장 적용\n",
    "token.word_counts        # word_counts에 결과 저장\n",
    "# 각 다언의 빈도수 : [(단어, 빈도수), ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b264a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 문장 수\n",
    "token.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ab880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'합니다': 1,\n",
       "             '텍스트의': 2,\n",
       "             '단어를': 1,\n",
       "             '토큰화': 3,\n",
       "             '먼저': 1,\n",
       "             '각': 1,\n",
       "             '나누어': 1,\n",
       "             '해야': 1,\n",
       "             '딥러닝에서': 2,\n",
       "             '인식합니다': 1,\n",
       "             '단어로': 1,\n",
       "             '수': 1,\n",
       "             '한': 1,\n",
       "             '있습니다': 1,\n",
       "             '사용할': 1,\n",
       "             '결과는': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 몇 개의 문장에 소속되어 있느지 (포함되어 있는지) : 소속된 문장 수\n",
    "token.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377eb413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 2), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식합니다', 1), ('한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
      "\n",
      "defaultdict(<class 'int'>, {'합니다': 1, '텍스트의': 2, '단어를': 1, '토큰화': 3, '먼저': 1, '각': 1, '나누어': 1, '해야': 1, '딥러닝에서': 2, '인식합니다': 1, '단어로': 1, '수': 1, '한': 1, '있습니다': 1, '사용할': 1, '결과는': 1})\n"
     ]
    }
   ],
   "source": [
    "print(token.word_counts) # 빈도수\n",
    "print()\n",
    "print(token.word_docs)  # 소속된 문장수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe3113",
   "metadata": {},
   "source": [
    "### 단어의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a022818",
   "metadata": {},
   "outputs": [],
   "source": [
    "단어의 원-핫 인코딩\n",
    "- 먼저 벡터 공간을 0으로 채우고\n",
    "- index에 해당되는 값만 1로 변경\n",
    "\n",
    "원-핫 인코딩 전에 각 단어에 index 부여 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18cba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "token.word_index  # 각 단어와 인덱스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94aa2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어의 index로만 이루어진 행렬 생성\n",
    "# texts_to_sequences() 메서드 사용\n",
    "x = token.texts_to_sequences([text])\n",
    "x\n",
    "# index가 1부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a1c1fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원-핫 인코딩 수행\n",
    "# to_categorical() 함수 사용\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "word_size = len(token.word_index) + 1\n",
    "x = to_categorical(x, num_classes=word_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b624c51",
   "metadata": {},
   "source": [
    "### 단어 임베딩 (Word Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "언-핫 인코딩의 문제\n",
    "- 원-핫 인코딩을 그대로 사용하면 벡터 길이가 너무 길어진다는 단점\n",
    "- 예로 1만개의 단어 토큰으로 이루어진 말뭉치를 다룬다고 할 때\n",
    "- 원-핫 인코딩을 적용해서 벡터화할 경우\n",
    "- 단어의 의미는 전혀 고려하지 않고\n",
    "- 9,999개의 0과 하나의 1로 이루어진 벡터를 1만개 만들어야 함\n",
    "- 공간적 낭비\n",
    "\n",
    "말뭉치(Corpus)\n",
    "- 자연어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합\n",
    "\n",
    "밀집 표현(Dense Representation)\n",
    "- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "- 이 과정에서 0과 1만 가진 값이 아니라 실수값을 가지게 됨\n",
    "- 예: \n",
    "    강아지 = [0 0 0 1..... 0] # 10,000 차원\n",
    "    \n",
    "    밀집 표현으로 변경\n",
    "    강아지 = [0.2 1.8 1.1....] # 128 차원\n",
    "    벡터의 차원이 조밀해졌다고 해서 밀집 벡터(dense vector)라고 함\n",
    "    \n",
    "단어 임베딩 \n",
    "- 공간적 낭비를 해결하기 위한 방법으로\n",
    "- 단어의 의미를 고려하고 밀집 벡터의 형태로 표현\n",
    "- 의미가 비슷한 단어는 비슷한 방향에 위치\n",
    "- 단어의 의미를 효과적으로 표현하기 때문에\n",
    "- 원-핫 인코딩보다 학습 성능을 높일 수 있음\n",
    "- 주어진 배열을 정해진 길이로 압축\n",
    "\n",
    "예: \n",
    "    단어 happy는 bad 보다 good에 더 가깝고\n",
    "    cat은 good 보다는 dog에 가깝다는 것을 고려하여\n",
    "    배열을 새로운 수치로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254af143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "케라스의 Embedding 클래스 사용하여 임베딩 층 구현\n",
    "Embedding(총 단어수(단어 집합의 크기),\n",
    "         임베딩 후 출력되는 벡터 크기,\n",
    "         input_length(각 입력 시퀀스 길이))\n",
    "Embedding(16, 4, input_length=2)\n",
    "\n",
    "임베딩 층 구현\n",
    "model = Sequential()\n",
    "model.add(Embedding(16, 4, input_length=2))\n",
    "\n",
    "단어 -> 정수값 변환 -> 임베딩 층 통과 -> 밀집 벡터\n",
    "\n",
    "임베딩 층\n",
    "- 입력 정수에 대해 밀집 벡터로 맵핑하고\n",
    "- 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련\n",
    "- 훈련 과정에서 단어는 모델이 해결하고자 하는 작업에 맞는 값으로 업데이트 됨\n",
    "- 이 밀집 벡터를 임베딩 벡터라고 함\n",
    "\n",
    "임베딩 층의 출력 크기\n",
    "model.add(Embedding(16, 4, input_length=2)) : 3D 실수형 텐서를 반환\n",
    "    \n",
    "Flatten 층 추가 : 3D 실수형 텐서를 2D로 변환 \n",
    "- 다음의 Dense()에서 처리할 수 있는 입력으로 변환\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2afe0",
   "metadata": {},
   "source": [
    "### 패딩 (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "패딩 (padding)\n",
    "- 문장의 길이를 나타내는 배열의 크기를 동일하게 맞추는 작업\n",
    "- 길이가 전부 동일한 문서들에 대하여 하나의 행렬로 인식하고\n",
    "- 한꺼번에 묶어서 처리할 수 있도록 하기 위함\n",
    "- 각 문장(문서)의 길이가 서로 다른 경우\n",
    "- 동일한 길이로 맞추고 빈 부분은 0으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4cece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'지금은': 1, '딥러닝을': 2, '공부하고': 3, '있어요': 4, '어렵지': 5, '않고': 6, '쉬워요': 7, '참': 8, '재미있어요': 9}\n"
     ]
    }
   ],
   "source": [
    "# 패딩 (padding)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentenses = ['지금은 딥러닝을 공부하고 있어요', '어렵지 않고 쉬워요', '참 재미있어요']\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(sentenses)\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8341c0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7], [8, 9]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장에 해당되는 인덱스를 배열로 생성\n",
    "result = token.texts_to_sequences(sentenses)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93a0b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [0, 5, 6, 7],\n",
       "       [0, 0, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩 : 각 배열의 길이를 동일하게 맞춤\n",
    "# pad_sequences() 함수 사용\n",
    "# 길이를 지정하지 않으면 제일 긴 배열의 크기에 맞춤\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(result)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3cf77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3, 4],\n",
       "       [0, 0, 0, 5, 6, 7],\n",
       "       [0, 0, 0, 0, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이를 6으로 지정\n",
    "padded = pad_sequences(result, 6) # maxlen=6\n",
    "padded\n",
    "# 디폴트 :  padding='pre' (앞 부분을 0으로 채움)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cea1d4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 0, 0],\n",
       "       [5, 6, 7, 0, 0, 0],\n",
       "       [8, 9, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding='post' (뒷 부분을 0으로 채움)\n",
    "padded = pad_sequences(result, padding='post', maxlen=6) # maxlen=6\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc1b5f",
   "metadata": {},
   "source": [
    "### 자연어 처리 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "텍스트를 읽고 긍정, 부정 예측하기\n",
    "- 영화 리뷰를 딥러닝 모델로 학습해서\n",
    "- 각 리뷰가 긍정적인지 부정적인지 예측\n",
    "\n",
    "예측 과정\n",
    "(1) 짧은 리뷰 10개를 불러와서\n",
    "- 긍적적이면 1, 부정적이면 0 클래스로 지정\n",
    "(2) 토큰화\n",
    "(3) 패딩\n",
    "(4) 임베딩 및 딥러닝 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0e0499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a99cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 짧은 리뷰 10개를 불러와서\n",
    "\n",
    "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\", \\\n",
    "        \"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\", \\\n",
    "        \"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
    "\n",
    "# 긍적적이면 1, 부정적이면 0 클래스로 지정\n",
    "classes = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57dd96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
     ]
    }
   ],
   "source": [
    "# (2) 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc2abd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'너무': 1,\n",
       " '재밌네요': 2,\n",
       " '최고예요': 3,\n",
       " '참': 4,\n",
       " '잘': 5,\n",
       " '만든': 6,\n",
       " '영화예요': 7,\n",
       " '추천하고': 8,\n",
       " '싶은': 9,\n",
       " '영화입니다': 10,\n",
       " '한번': 11,\n",
       " '더': 12,\n",
       " '보고싶네요': 13,\n",
       " '글쎄요': 14,\n",
       " '별로예요': 15,\n",
       " '생각보다': 16,\n",
       " '지루하네요': 17,\n",
       " '연기가': 18,\n",
       " '어색해요': 19,\n",
       " '재미없어요': 20}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "126f8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
     ]
    }
   ],
   "source": [
    "# 각 문장에 해당되는 인덱스를 배열로 생성\n",
    "x = token.texts_to_sequences(docs)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8a53658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      " [[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [ 0 11 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "# (3) 패딩 : 길이를 4로 맞춤\n",
    "padded_x = pad_sequences(x, 4)\n",
    "print('패딩 결과 :\\n', padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "딥러닝 모델 적용\n",
    "(1) 모델 설정\n",
    "(2) 모델 컴파일\n",
    "(3) 모델 훈련(학습)\n",
    "(4) 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52e3bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩에 입력할 단어 수 지정\n",
    "# 맨 앞에 0이 나오도록 len() + 1\n",
    "word_size = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77fc9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 817ms/step - loss: 0.6968 - accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.4000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6900 - accuracy: 0.4000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6878 - accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6856 - accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6834 - accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6812 - accuracy: 0.7000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.7000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6768 - accuracy: 0.7000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6746 - accuracy: 0.8000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6724 - accuracy: 0.9000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.9000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6681 - accuracy: 0.9000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6659 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6616 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6594 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6572 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6550 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.6528 - accuracy: 1.0000\n",
      "\n",
      " Accuracy : 1.0000\n"
     ]
    }
   ],
   "source": [
    "# (1) 모델 설정\n",
    "# Embedding 층 구현\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4)) # 3D 텐석\n",
    "model.add(Flatten())  # 2D로 변환\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# (2) 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# (3) 모델 훈련(학습)\n",
    "model.fit(padded_x, classes, epochs=20)\n",
    "\n",
    "# (4) 모델 평가\n",
    "print('\\n Accuracy : %.4f' % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4475e2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n",
      "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n",
      "패딩 결과 :\n",
      " [[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [ 0 11 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  0 20]]\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 723ms/step - loss: 0.6918 - accuracy: 0.7000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6880 - accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6843 - accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6824 - accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6805 - accuracy: 0.9000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6787 - accuracy: 0.9000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6768 - accuracy: 0.9000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6731 - accuracy: 0.9000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6712 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6693 - accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6674 - accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6655 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.8000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.8000\n",
      "WARNING:tensorflow:7 out of the last 10 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2e6833c560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.6541 - accuracy: 0.8000\n",
      "\n",
      " Accuracy : 0.8000\n"
     ]
    }
   ],
   "source": [
    "# 전체 과정 하나로 합치기\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# (1) 짧은 리뷰 10개를 불러와서\n",
    "\n",
    "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\", \\\n",
    "        \"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\", \\\n",
    "        \"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
    "\n",
    "# 긍적적이면 1, 부정적이면 0 클래스로 지정\n",
    "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# (2) 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n",
    "\n",
    "# 각 문장에 해당되는 인덱스를 배열로 생성\n",
    "x = token.texts_to_sequences(docs)\n",
    "print(x)\n",
    "\n",
    "padded_x = pad_sequences(x, 4)\n",
    "print('패딩 결과 :\\n', padded_x)\n",
    "\n",
    "# 임베딩에 입력할 단어 수 지정\n",
    "# 맨 앞에 0이 나오도록 len() + 1\n",
    "word_size = len(token.word_index) + 1\n",
    "\n",
    "# 딥러닝 적용\n",
    "# (1) 모델 설정\n",
    "# Embedding 층 구현\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4)) # 3D 텐서 반환\n",
    "model.add(Flatten())  # 2D로 변환\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# (2) 모델 컴파일\n",
    "model.compile(optimizer='adam',  # rmsprop, adam\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# (3) 모델 훈련(학습)\n",
    "model.fit(padded_x, classes, epochs=20)\n",
    "\n",
    "# (4) 모델 평가\n",
    "print('\\n Accuracy : %.4f' % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afa3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKD_ENV",
   "language": "python",
   "name": "hkdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
